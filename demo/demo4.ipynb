{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "56b09098",
   "metadata": {},
   "source": [
    "# demo 4 - kernel\n",
    "\n",
    "Compare performances of models based on \n",
    "1. fixed kernel\n",
    "2. the regular\n",
    "3. update kernel after every step of forward pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b81e8991",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "from iHMM_GP.step1_utils import *\n",
    "from iHMM_GP.data_setup import *\n",
    "#do not use these variable names: f0, f1, f2, g1, g2, g3, g5, f_new1, f_new2, f_new3\n",
    "\n",
    "T = 100\n",
    "n = 50\n",
    "sigma2 = 2\n",
    "\n",
    "T_test = 50\n",
    "\n",
    "# create data\n",
    "data, s = sim_new_data2(f_true, Pi_true, T=T+T_test, n=n, sigma2=sigma2)\n",
    "\n",
    "# get training data - for t in tau, hold out q% of the data\n",
    "data_train = []\n",
    "\n",
    "for t in range(T):\n",
    "    X, Y = data[t]\n",
    "    data_train.append((X, Y))\n",
    "    \n",
    "# test 3 - new sequence state prediction\n",
    "data_test3 = data[T:]\n",
    "    \n",
    "# state labels:\n",
    "s_train_true = s[0:T]\n",
    "s_test_true = s[T:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "357f4e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "from iHMM_GP.gp_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5e72fe51",
   "metadata": {},
   "outputs": [],
   "source": [
    "iHMM_params = (3,3,3)\n",
    "\n",
    "x, y = np.meshgrid(np.linspace(-1, 1, 10), np.linspace(-1, 1, 10))\n",
    "Z = np.array([[x[i, j], y[i, j]] for i in range(10) for j in range(10)])\n",
    "\n",
    "from sklearn.metrics import rand_score, adjusted_mutual_info_score, v_measure_score\n",
    "import time\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09e20c27",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9fdaaf21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fixed kernel\n",
    "kernel_bounds = [(1,1), (1,1)]\n",
    "noise_bounds = (1e-3, 1e3)\n",
    "time0 = time.time()\n",
    "\n",
    "marginal_lls, sigma2, all_kernels = get_all_GPs(data_train, k_ls_bounds=kernel_bounds[1], k_var_bounds=kernel_bounds[0])\n",
    "#marginal_lls, sigma2, all_kernels = get_all_GPs(data_train)\n",
    "k_params = [(1,1), (1,1)] \n",
    "\n",
    "s1, o1, models1, N1, M1 = forward_pass(data_train, iHMM_params, marginal_lls, all_kernels, sigma2, Z)\n",
    "s2, o2, models2, N2, M2 = refinement(s1, o1, N1, M1, models1, iHMM_params, sigma2, marginal_lls, data_train, min_cluster_size=3, verbose=False)\n",
    "\n",
    "logliks_train = get_likelihoods(data_train, models2, sigma2); logliks_train = np.array(logliks_train)\n",
    "Pi_est, _ = get_nm(s2, np.zeros(T)); row_sums = Pi_est.sum(axis=1)\n",
    "Pi_hat = Pi_est / row_sums[:, np.newaxis]\n",
    "s_final_train = viterbi(None, None, Pi_hat, logliks_train)\n",
    "\n",
    "# check if any cluster has 0 time points, can remove it\n",
    "s_final_train = rename_s(s_final_train)[1]\n",
    "K_opt = len(np.unique(s_final_train))\n",
    "\n",
    "final_models = get_final_results(data_train, s_final_train, sigma2, Z, n_jobs=-1, N_max=500) #new 2 lines\n",
    "\n",
    "time0 =time.time() - time0\n",
    "logliks_train = get_likelihoods(data_train, final_models, sigma2); logliks_train = np.array(logliks_train) #\n",
    "Pi_est, _ = get_nm(s_final_train, np.zeros(T)); row_sums = Pi_est.sum(axis=1)\n",
    "Pi_hat = Pi_est / row_sums[:, np.newaxis]\n",
    "logliks_test = get_likelihoods(data_test3, final_models, sigma2); logliks_test = np.array(logliks_test)\n",
    "s_final_test = viterbi(None, None, Pi_hat, logliks_test)\n",
    "\n",
    "hmm_loglik_train = compute_likelihood_HMM(Pi_hat, logliks_train)\n",
    "hmm_loglik_test = compute_likelihood_HMM(Pi_hat, logliks_test)\n",
    "\n",
    "\n",
    "train_label_acc = (rand_score(s_train_true, s_final_train), adjusted_mutual_info_score(s_train_true, s_final_train), v_measure_score(s_train_true, s_final_train))\n",
    "test_label_acc = (rand_score(s_test_true, s_final_test), adjusted_mutual_info_score(s_test_true, s_final_test), v_measure_score(s_test_true, s_final_test))\n",
    "\n",
    "obj = {'K': K_opt,\n",
    "     'hmm_loglik_train': hmm_loglik_train,\n",
    "     'hmm_loglik_test': hmm_loglik_test,\n",
    "     'train_label_accuracy': train_label_acc,\n",
    "     'test_label_accuracy': test_label_acc,\n",
    "     'time': time0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "055e400c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'K': 4,\n",
       " 'hmm_loglik_train': -18303.26485526144,\n",
       " 'hmm_loglik_test': -9184.423412348819,\n",
       " 'train_label_accuracy': (0.6513131313131313,\n",
       "  0.35966459591530375,\n",
       "  0.4124099641822997),\n",
       " 'test_label_accuracy': (0.603265306122449,\n",
       "  0.4648899009259502,\n",
       "  0.5544489247621441),\n",
       " 'time': 13.591920852661133}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0054834",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0ee3e8ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# regular\n",
    "# fixed kernel\n",
    "\n",
    "#kernel_bounds = [(1e-3, 1e3), (1e-3,1e3)]\n",
    "#noise_bounds = (1e-3, 1e3)\n",
    "time0 = time.time()\n",
    "\n",
    "#marginal_lls, sigma2, all_kernels = get_all_GPs(data_train, k_ls_bounds=kernel_bounds[1], k_var_bounds=kernel_bounds[0])\n",
    "marginal_lls, sigma2, all_kernels = get_all_GPs(data_train)\n",
    "#k_params = [(1,1), (1,1)] \n",
    "\n",
    "s1, o1, models1, N1, M1 = forward_pass(data_train, iHMM_params, marginal_lls, all_kernels, sigma2, Z, update_kernel_every=200)\n",
    "s2, o2, models2, N2, M2 = refinement(s1, o1, N1, M1, models1, iHMM_params, sigma2, marginal_lls, data_train, min_cluster_size=2, verbose=False)\n",
    "\n",
    "logliks_train = get_likelihoods(data_train, models2, sigma2); logliks_train = np.array(logliks_train)\n",
    "Pi_est, _ = get_nm(s2, np.zeros(T)); row_sums = Pi_est.sum(axis=1)\n",
    "Pi_hat = Pi_est / row_sums[:, np.newaxis]\n",
    "s_final_train = viterbi(None, None, Pi_hat, logliks_train)\n",
    "\n",
    "# check if any cluster has 0 time points, can remove it\n",
    "s_final_train = rename_s(s_final_train)[1]\n",
    "K_opt = len(np.unique(s_final_train))\n",
    "\n",
    "final_models = get_final_results(data_train, s_final_train, sigma2, Z, n_jobs=4, N_max=500) #new 2 lines\n",
    "\n",
    "time0 =time.time() - time0\n",
    "logliks_train = get_likelihoods(data_train, final_models, sigma2); logliks_train = np.array(logliks_train) #\n",
    "Pi_est, _ = get_nm(s_final_train, np.zeros(T)); row_sums = Pi_est.sum(axis=1)\n",
    "Pi_hat = Pi_est / row_sums[:, np.newaxis]\n",
    "logliks_test = get_likelihoods(data_test3, final_models, sigma2); logliks_test = np.array(logliks_test)\n",
    "s_final_test = viterbi(None, None, Pi_hat, logliks_test)\n",
    "\n",
    "hmm_loglik_train = compute_likelihood_HMM(Pi_hat, logliks_train)\n",
    "hmm_loglik_test = compute_likelihood_HMM(Pi_hat, logliks_test)\n",
    "\n",
    "\n",
    "train_label_acc = (rand_score(s_train_true, s_final_train), adjusted_mutual_info_score(s_train_true, s_final_train), v_measure_score(s_train_true, s_final_train))\n",
    "test_label_acc = (rand_score(s_test_true, s_final_test), adjusted_mutual_info_score(s_test_true, s_final_test), v_measure_score(s_test_true, s_final_test))\n",
    "\n",
    "obj = {'K': K_opt,\n",
    "     'hmm_loglik_train': hmm_loglik_train,\n",
    "     'hmm_loglik_test': hmm_loglik_test,\n",
    "     'train_label_accuracy': train_label_acc,\n",
    "     'test_label_accuracy': test_label_acc,\n",
    "     'time': time0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6342005d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'K': 7,\n",
       " 'hmm_loglik_train': -17663.572826106414,\n",
       " 'hmm_loglik_test': -9023.088120835848,\n",
       " 'train_label_accuracy': (0.9278787878787879,\n",
       "  0.7847737675212959,\n",
       "  0.8152667237553921),\n",
       " 'test_label_accuracy': (0.9493877551020408,\n",
       "  0.8659202870903,\n",
       "  0.9037584753685233),\n",
       " 'time': 28.077348709106445}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f57501f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e0dd2c63",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "63da8cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# update every 10\n",
    "# fixed kernel\n",
    "kernel_bounds = [(1e-3, 1e3), (1e-3,1e3)]\n",
    "noise_bounds = (1e-3, 1e3)\n",
    "time0 = time.time()\n",
    "\n",
    "marginal_lls, sigma2, all_kernels = get_all_GPs(data_train, k_ls_bounds=kernel_bounds[1], k_var_bounds=kernel_bounds[0])\n",
    "k_params = [(1,1), (1,1)] \n",
    "\n",
    "s1, o1, models1, N1, M1 = forward_pass(data_train, iHMM_params, marginal_lls, all_kernels, sigma2, Z, update_kernel_every=5)\n",
    "s2, o2, models2, N2, M2 = refinement(s1, o1, N1, M1, models1, iHMM_params, sigma2, marginal_lls, data_train, min_cluster_size=3, verbose=False)\n",
    "\n",
    "logliks_train = get_likelihoods(data_train, models2, sigma2); logliks_train = np.array(logliks_train)\n",
    "Pi_est, _ = get_nm(s2, np.zeros(T)); row_sums = Pi_est.sum(axis=1)\n",
    "Pi_hat = Pi_est / row_sums[:, np.newaxis]\n",
    "s_final_train = viterbi(None, None, Pi_hat, logliks_train)\n",
    "\n",
    "# check if any cluster has 0 time points, can remove it\n",
    "s_final_train = rename_s(s_final_train)[1]\n",
    "K_opt = len(np.unique(s_final_train))\n",
    "\n",
    "final_models = get_final_results(data_train, s_final_train, sigma2, Z, n_jobs=4, N_max=500) #new 2 lines\n",
    "\n",
    "time0 =time.time() - time0\n",
    "logliks_train = get_likelihoods(data_train, final_models, sigma2); logliks_train = np.array(logliks_train) #\n",
    "Pi_est, _ = get_nm(s_final_train, np.zeros(T)); row_sums = Pi_est.sum(axis=1)\n",
    "Pi_hat = Pi_est / row_sums[:, np.newaxis]\n",
    "logliks_test = get_likelihoods(data_test3, final_models, sigma2); logliks_test = np.array(logliks_test)\n",
    "s_final_test = viterbi(None, None, Pi_hat, logliks_test)\n",
    "\n",
    "hmm_loglik_train = compute_likelihood_HMM(Pi_hat, logliks_train)\n",
    "hmm_loglik_test = compute_likelihood_HMM(Pi_hat, logliks_test)\n",
    "\n",
    "\n",
    "train_label_acc = (rand_score(s_train_true, s_final_train), adjusted_mutual_info_score(s_train_true, s_final_train), v_measure_score(s_train_true, s_final_train))\n",
    "test_label_acc = (rand_score(s_test_true, s_final_test), adjusted_mutual_info_score(s_test_true, s_final_test), v_measure_score(s_test_true, s_final_test))\n",
    "\n",
    "obj = {'K': K_opt,\n",
    "     'hmm_loglik_train': hmm_loglik_train,\n",
    "     'hmm_loglik_test': hmm_loglik_test,\n",
    "     'train_label_accuracy': train_label_acc,\n",
    "     'test_label_accuracy': test_label_acc,\n",
    "     'time': time0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0e57c94e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'K': 8,\n",
       " 'hmm_loglik_train': -17907.732061929346,\n",
       " 'hmm_loglik_test': -9089.432503732034,\n",
       " 'train_label_accuracy': (0.8840404040404041,\n",
       "  0.6237863119189498,\n",
       "  0.6810117918112812),\n",
       " 'test_label_accuracy': (0.893061224489796,\n",
       "  0.5850042362141471,\n",
       "  0.7036056191633112),\n",
       " 'time': 88.92849802970886}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ccd5fe8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cc37df4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7b51745e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simul(T=100, n=50, sigma2=2):\n",
    "    \n",
    "    # set up data\n",
    "    T_test = 50\n",
    "\n",
    "    # create data\n",
    "    data, s = sim_new_data2(f_true, Pi_true, T=T+T_test, n=n, sigma2=sigma2)\n",
    "\n",
    "    # get training data - for t in tau, hold out q% of the data\n",
    "    data_train = []\n",
    "\n",
    "    for t in range(T):\n",
    "        X, Y = data[t]\n",
    "        data_train.append((X, Y))\n",
    "\n",
    "    # test 3 - new sequence state prediction\n",
    "    data_test3 = data[T:]\n",
    "\n",
    "    # state labels:\n",
    "    s_train_true = s[0:T]\n",
    "    s_test_true = s[T:]\n",
    "    \n",
    "    iHMM_params = (3,3,3)\n",
    "\n",
    "    x, y = np.meshgrid(np.linspace(-1, 1, 10), np.linspace(-1, 1, 10))\n",
    "    Z = np.array([[x[i, j], y[i, j]] for i in range(10) for j in range(10)])\n",
    "\n",
    "    from sklearn.metrics import rand_score, adjusted_mutual_info_score, v_measure_score\n",
    "    import time\n",
    "    \n",
    "    ## METHOD 1\n",
    "    kernel_bounds = [(1,1), (1,1)]\n",
    "    noise_bounds = (1e-3, 1e3)\n",
    "    time0 = time.time()\n",
    "\n",
    "    marginal_lls, sigma2, all_kernels = get_all_GPs(data_train, k_ls_bounds=kernel_bounds[1], k_var_bounds=kernel_bounds[0])\n",
    "    k_params = [(1,1), (1,1)] \n",
    "\n",
    "    s1, o1, models1, N1, M1 = forward_pass(data_train, iHMM_params, marginal_lls, all_kernels, sigma2, Z, update_kernel_every=200)\n",
    "    s2, o2, models2, N2, M2 = refinement(s1, o1, N1, M1, models1, iHMM_params, sigma2, marginal_lls, data_train, min_cluster_size=3, verbose=False)\n",
    "\n",
    "    logliks_train = get_likelihoods(data_train, models2, sigma2); logliks_train = np.array(logliks_train)\n",
    "    Pi_est, _ = get_nm(s2, np.zeros(T)); row_sums = Pi_est.sum(axis=1)\n",
    "    Pi_hat = Pi_est / row_sums[:, np.newaxis]\n",
    "    s_final_train = viterbi(None, None, Pi_hat, logliks_train)\n",
    "\n",
    "    # check if any cluster has 0 time points, can remove it\n",
    "    s_final_train = rename_s(s_final_train)[1]\n",
    "    K_opt = len(np.unique(s_final_train))\n",
    "\n",
    "    final_models = get_final_results(data_train, s_final_train, sigma2, Z, n_jobs=-1, N_max=500) #new 2 lines\n",
    "\n",
    "    time0 =time.time() - time0\n",
    "    logliks_train = get_likelihoods(data_train, final_models, sigma2); logliks_train = np.array(logliks_train) #\n",
    "    Pi_est, _ = get_nm(s_final_train, np.zeros(T)); row_sums = Pi_est.sum(axis=1)\n",
    "    Pi_hat = Pi_est / row_sums[:, np.newaxis]\n",
    "    logliks_test = get_likelihoods(data_test3, final_models, sigma2); logliks_test = np.array(logliks_test)\n",
    "    s_final_test = viterbi(None, None, Pi_hat, logliks_test)\n",
    "\n",
    "    hmm_loglik_train = compute_likelihood_HMM(Pi_hat, logliks_train)\n",
    "    hmm_loglik_test = compute_likelihood_HMM(Pi_hat, logliks_test)\n",
    "\n",
    "\n",
    "    train_label_acc = (rand_score(s_train_true, s_final_train), adjusted_mutual_info_score(s_train_true, s_final_train), v_measure_score(s_train_true, s_final_train))\n",
    "    test_label_acc = (rand_score(s_test_true, s_final_test), adjusted_mutual_info_score(s_test_true, s_final_test), v_measure_score(s_test_true, s_final_test))\n",
    "\n",
    "    obj1 = {'K': K_opt,\n",
    "         'hmm_loglik_train': hmm_loglik_train,\n",
    "         'hmm_loglik_test': hmm_loglik_test,\n",
    "         'train_label_accuracy': train_label_acc,\n",
    "         'test_label_accuracy': test_label_acc,\n",
    "         'time': time0}\n",
    "\n",
    "    ## METHOD 2\n",
    "    #kernel_bounds = [(1e-3, 1e3), (1e-3,1e3)]\n",
    "    #noise_bounds = (1e-3, 1e3)\n",
    "    time0 = time.time()\n",
    "\n",
    "    marginal_lls, sigma2, all_kernels = get_all_GPs(data_train)\n",
    "    k_params = [(1,1), (1,1)] \n",
    "\n",
    "    s1, o1, models1, N1, M1 = forward_pass(data_train, iHMM_params, marginal_lls, all_kernels, sigma2, Z, update_kernel_every=200)\n",
    "    s2, o2, models2, N2, M2 = refinement(s1, o1, N1, M1, models1, iHMM_params, sigma2, marginal_lls, data_train, min_cluster_size=3, verbose=False)\n",
    "\n",
    "    logliks_train = get_likelihoods(data_train, models2, sigma2); logliks_train = np.array(logliks_train)\n",
    "    Pi_est, _ = get_nm(s2, np.zeros(T)); row_sums = Pi_est.sum(axis=1)\n",
    "    Pi_hat = Pi_est / row_sums[:, np.newaxis]\n",
    "    s_final_train = viterbi(None, None, Pi_hat, logliks_train)\n",
    "\n",
    "    # check if any cluster has 0 time points, can remove it\n",
    "    s_final_train = rename_s(s_final_train)[1]\n",
    "    K_opt = len(np.unique(s_final_train))\n",
    "\n",
    "    final_models = get_final_results(data_train, s_final_train, sigma2, Z, n_jobs=-1, N_max=500) #new 2 lines\n",
    "\n",
    "    time0 =time.time() - time0\n",
    "    logliks_train = get_likelihoods(data_train, final_models, sigma2); logliks_train = np.array(logliks_train) #\n",
    "    Pi_est, _ = get_nm(s_final_train, np.zeros(T)); row_sums = Pi_est.sum(axis=1)\n",
    "    Pi_hat = Pi_est / row_sums[:, np.newaxis]\n",
    "    logliks_test = get_likelihoods(data_test3, final_models, sigma2); logliks_test = np.array(logliks_test)\n",
    "    s_final_test = viterbi(None, None, Pi_hat, logliks_test)\n",
    "\n",
    "    hmm_loglik_train = compute_likelihood_HMM(Pi_hat, logliks_train)\n",
    "    hmm_loglik_test = compute_likelihood_HMM(Pi_hat, logliks_test)\n",
    "\n",
    "\n",
    "    train_label_acc = (rand_score(s_train_true, s_final_train), adjusted_mutual_info_score(s_train_true, s_final_train), v_measure_score(s_train_true, s_final_train))\n",
    "    test_label_acc = (rand_score(s_test_true, s_final_test), adjusted_mutual_info_score(s_test_true, s_final_test), v_measure_score(s_test_true, s_final_test))\n",
    "\n",
    "    obj2 = {'K': K_opt,\n",
    "         'hmm_loglik_train': hmm_loglik_train,\n",
    "         'hmm_loglik_test': hmm_loglik_test,\n",
    "         'train_label_accuracy': train_label_acc,\n",
    "         'test_label_accuracy': test_label_acc,\n",
    "         'time': time0}\n",
    "    \n",
    "    ## METHOD 3\n",
    "    #kernel_bounds = [(1e-3, 1e3), (1e-3,1e3)]\n",
    "    #noise_bounds = (1e-3, 1e3)\n",
    "    time0 = time.time()\n",
    "\n",
    "    marginal_lls, sigma2, all_kernels = get_all_GPs(data_train)\n",
    "    k_params = [(1,1), (1,1)] \n",
    "\n",
    "    s1, o1, models1, N1, M1 = forward_pass(data_train, iHMM_params, marginal_lls, all_kernels, sigma2, Z, update_kernel_every=5)\n",
    "    s2, o2, models2, N2, M2 = refinement(s1, o1, N1, M1, models1, iHMM_params, sigma2, marginal_lls, data_train, min_cluster_size=3, verbose=False)\n",
    "\n",
    "    logliks_train = get_likelihoods(data_train, models2, sigma2); logliks_train = np.array(logliks_train)\n",
    "    Pi_est, _ = get_nm(s2, np.zeros(T)); row_sums = Pi_est.sum(axis=1)\n",
    "    Pi_hat = Pi_est / row_sums[:, np.newaxis]\n",
    "    s_final_train = viterbi(None, None, Pi_hat, logliks_train)\n",
    "\n",
    "    # check if any cluster has 0 time points, can remove it\n",
    "    s_final_train = rename_s(s_final_train)[1]\n",
    "    K_opt = len(np.unique(s_final_train))\n",
    "\n",
    "    final_models = get_final_results(data_train, s_final_train, sigma2, Z, n_jobs=-1, N_max=500) #new 2 lines\n",
    "\n",
    "    time0 =time.time() - time0\n",
    "    logliks_train = get_likelihoods(data_train, final_models, sigma2); logliks_train = np.array(logliks_train) #\n",
    "    Pi_est, _ = get_nm(s_final_train, np.zeros(T)); row_sums = Pi_est.sum(axis=1)\n",
    "    Pi_hat = Pi_est / row_sums[:, np.newaxis]\n",
    "    logliks_test = get_likelihoods(data_test3, final_models, sigma2); logliks_test = np.array(logliks_test)\n",
    "    s_final_test = viterbi(None, None, Pi_hat, logliks_test)\n",
    "\n",
    "    hmm_loglik_train = compute_likelihood_HMM(Pi_hat, logliks_train)\n",
    "    hmm_loglik_test = compute_likelihood_HMM(Pi_hat, logliks_test)\n",
    "\n",
    "\n",
    "    train_label_acc = (rand_score(s_train_true, s_final_train), adjusted_mutual_info_score(s_train_true, s_final_train), v_measure_score(s_train_true, s_final_train))\n",
    "    test_label_acc = (rand_score(s_test_true, s_final_test), adjusted_mutual_info_score(s_test_true, s_final_test), v_measure_score(s_test_true, s_final_test))\n",
    "\n",
    "    obj3 = {'K': K_opt,\n",
    "         'hmm_loglik_train': hmm_loglik_train,\n",
    "         'hmm_loglik_test': hmm_loglik_test,\n",
    "         'train_label_accuracy': train_label_acc,\n",
    "         'test_label_accuracy': test_label_acc,\n",
    "         'time': time0}\n",
    "    return (obj1, obj2, obj3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d8c9fd79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'K': 5,\n",
       "  'hmm_loglik_train': -21909.939800983,\n",
       "  'hmm_loglik_test': -9206.177934056244,\n",
       "  'train_label_accuracy': (0.6509803921568628,\n",
       "   0.44387932941182623,\n",
       "   0.4953250485457449),\n",
       "  'test_label_accuracy': (0.6277551020408163,\n",
       "   0.4677119017122544,\n",
       "   0.5608551034233984),\n",
       "  'time': 9.68434190750122},\n",
       " {'K': 10,\n",
       "  'hmm_loglik_train': -21093.391501214202,\n",
       "  'hmm_loglik_test': -8969.08983115837,\n",
       "  'train_label_accuracy': (0.9592436974789916,\n",
       "   0.8511422246088863,\n",
       "   0.8742497251176026),\n",
       "  'test_label_accuracy': (0.9918367346938776,\n",
       "   0.962335553197468,\n",
       "   0.9753099470389273),\n",
       "  'time': 35.32162404060364},\n",
       " {'K': 8,\n",
       "  'hmm_loglik_train': -21087.443041259787,\n",
       "  'hmm_loglik_test': -8922.153469472501,\n",
       "  'train_label_accuracy': (0.9572829131652661,\n",
       "   0.88401066078267,\n",
       "   0.8993795812045308),\n",
       "  'test_label_accuracy': (0.9755102040816327,\n",
       "   0.9476564313664771,\n",
       "   0.964148938177539),\n",
       "  'time': 87.07919692993164})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simul(T=120, n=50, sigma2=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c5966898",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "working at  0\n",
      "working at  1\n",
      "working at  2\n",
      "working at  3\n",
      "working at  4\n",
      "working at  5\n",
      "working at  6\n",
      "working at  7\n",
      "working at  8\n",
      "working at  9\n",
      "working at  10\n",
      "working at  11\n",
      "working at  12\n",
      "working at  13\n",
      "working at  14\n",
      "working at  15\n",
      "working at  16\n",
      "working at  17\n",
      "working at  18\n",
      "working at  19\n",
      "working at  20\n",
      "working at  21\n",
      "working at  22\n",
      "working at  23\n",
      "working at  24\n",
      "working at  25\n",
      "working at  26\n",
      "working at  27\n",
      "working at  28\n",
      "working at  29\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "rep = 30\n",
    "\n",
    "for i in range(rep):\n",
    "    print('working at ', i)\n",
    "    res = simul(T=120, n=50, sigma2=2)\n",
    "    results.append(res)\n",
    "    \n",
    "pickle.dump(results, open( \"new_simulation_results/demo4.p\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1dbbe002",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(results, open( \"results/demo4.p\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1981e880",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
